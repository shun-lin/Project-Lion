{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ColabTesting.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"KpjymiON07Zh","outputId":"45ac1682-2464-4ae0-ad14-3dc48232fb23","executionInfo":{"status":"ok","timestamp":1556856650940,"user_tz":420,"elapsed":4040,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!pip install segtok"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: segtok in /usr/local/lib/python3.6/dist-packages (1.5.7)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from segtok) (2018.1.10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AvW_fh-ewHe0","colab_type":"code","outputId":"2d3b7606-6635-41d7-d3a1-80a8c4afb950","executionInfo":{"status":"ok","timestamp":1556857137561,"user_tz":420,"elapsed":511,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Getting access to the dataset and the Python files on Google Drive.\n","# You will probably have to give permission.\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","root_folder = \"/content/gdrive/My Drive/Project-Lion/\""],"execution_count":10,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uFTop3zAwKnB","colab_type":"code","colab":{}},"source":["from segtok import tokenizer\n","from collections import Counter\n","import tensorflow as tf\n","import numpy as np\n","import json\n","import os, sys\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import string\n","\n","sys.path.insert(0, \"/content/gdrive/My Drive/Project-Lion/\") # This enables us to import Python libraries in the folder.\n","\n","\n","root_folder = \"/content/gdrive/My Drive/Project-Lion/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXc9ApkzwcIG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EuazzbKMwhmV","colab_type":"text"},"source":["Preprocessing"]},{"cell_type":"code","metadata":{"id":"iTZmqo2Qwkqx","colab_type":"code","colab":{}},"source":["def numerize_sequence(tokenized):\n","    return [w2i.get(w, unkI) for w in tokenized]\n","def pad_sequence(numerized, pad_index, to_length):\n","    pad = numerized[:to_length]\n","    padded = pad + [pad_index] * (to_length - len(pad))\n","    mask = [w != pad_index for w in padded]\n","    return padded, mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bh6h3Rpt1EQD","colab_type":"code","colab":{}},"source":["dataset_df = pd.read_csv(root_folder + 'data/kaggle/shortjokes.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtWy-roR48-l","colab_type":"code","colab":{}},"source":["def clean_data(lst, in_place=True, keep_punc=\"\"):\n","    if in_place:\n","        new_lst = lst\n","    else:\n","        new_lst = lst.copy()\n","        \n","    for i in range(len(new_lst)):\n","        joke = lst[i]\n","        joke = joke.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"/\", \" or \")\n","        joke = joke.translate(str.maketrans(\"\", \"\", string.punctuation.replace(keep_punc, \"\"))) # Remove punctuation\n","        joke = joke.lower() # Lowercase\n","        joke = ''.join(char for char in joke if not char.isdigit()) # Remove numbers\n","        joke = joke.strip() # Remove leading and ending whitespace\n","        new_lst[i] = joke\n","    return new_lst"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-GYmsN6g5uiY","colab_type":"code","colab":{}},"source":["dataset_df = dataset_df.loc[dataset_df[\"Joke\"] != \"\"]\n","#dataset_df = dataset_df[[\"Joke\"]]\n","#dataset_df = clean_data(dataset_df[\"Joke\"].tolist(), keep_punc=\"?\")\n","dataset = dataset_df.to_dict('records')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5vmtGdAwlDk","colab_type":"code","outputId":"9deeb369-9cd8-425c-b26f-51f164a900a9","executionInfo":{"status":"ok","timestamp":1556857598771,"user_tz":420,"elapsed":21413,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# You do not need to run this\n","# This is to show you how the dataset was created\n","# You should read to understand, so you can preprocess text\n","# In the same way, in the evaluation section\n","\n","input_length = 0\n","\n","print(type(dataset))\n","for a in dataset:\n","    tokenized_joke = tokenizer.word_tokenizer(a['Joke'].lower())\n","    input_length = max(input_length, len(tokenized_joke))\n","    a['tokenized'] = tokenized_joke\n","print(input_length)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["<class 'list'>\n","96\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"poV73krUwpQ2","colab_type":"code","outputId":"352e7e14-44ff-42ee-f93e-2169ec070b73","executionInfo":{"status":"ok","timestamp":1556857600141,"user_tz":420,"elapsed":21130,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# You do not need to run this\n","# This is to show you how the dataset was created\n","# You should read to understand, so you can preprocess text\n","# In the same way, in the evaluation section\n","\n","word_counts = Counter()\n","for a in dataset:\n","    word_counts.update(a['tokenized'])\n","\n","print(word_counts.most_common(30))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[('a', 170819), ('the', 154009), ('.', 137049), ('?', 108416), ('i', 97247), (',', 91799), ('to', 87971), ('you', 79574), ('\"', 78575), ('and', 59807), ('in', 51550), ('of', 49315), ('my', 48889), ('what', 45030), (':', 44404), ('is', 43020), ('it', 40819), ('do', 36100), ('...', 34400), ('me', 31167), ('!', 28826), ('on', 28000), ('was', 27008), ('for', 26258), ('that', 25307), ('with', 24181), ('have', 23592), ('why', 22874), ('he', 22618), ('your', 20628)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GIDHNVVbTg0L","colab_type":"code","outputId":"7ea9d2c7-921c-400e-c165-1356d5f534ab","executionInfo":{"status":"ok","timestamp":1556857600143,"user_tz":420,"elapsed":18164,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["len(word_counts)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["89800"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"fa3QiS3ewzno","colab_type":"code","outputId":"d3e3e755-025d-4cd0-e316-385f7e6af670","executionInfo":{"status":"ok","timestamp":1556857623073,"user_tz":420,"elapsed":8219,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# You do not need to run this\n","# This is to show you how the dataset was created\n","# You should read to understand, so you can preprocess text\n","# In the same way, in the evaluation section\n","\n","# Creating the vocab\n","vocab_size = len(word_counts)\n","special_words = [\"<START>\", \"UNK\", \"PAD\"]\n","vocabulary = special_words + [w for w, c in word_counts.most_common(vocab_size-len(special_words))]\n","w2i = {w: i for i, w in enumerate(vocabulary)}\n","\n","# Numerizing and padding\n","unkI, padI, startI = w2i['UNK'], w2i['PAD'], w2i['<START>']\n","\n","for a in dataset:\n","    a['numerized'] = numerize_sequence(a['tokenized']) # Change words to IDs\n","    a['numerized'], a['mask'] = pad_sequence(a['numerized'], padI, input_length) # Append appropriate PAD tokens\n","    \n","# Compute fraction of words that are UNK:\n","word_counters = Counter([w for a in dataset for w in a['Joke'] if w != padI])\n","\n","print(\"Fraction of UNK words:\", float(word_counters[unkI]) / sum(word_counters.values()))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Fraction of UNK words: 0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rx2JJABkw2bg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xy06o5Utw26g","colab_type":"code","outputId":"b9e60777-a060-4f37-bfda-3889ce4cc8f9","executionInfo":{"status":"ok","timestamp":1556857627103,"user_tz":420,"elapsed":573,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["vocab_size = len(vocabulary)\n","input_length = len(dataset[0]['numerized']) # The length of the first element in the dataset, they are all of the same length\n","\n","d_train, d_valid = train_test_split(dataset, test_size=0.01, random_state=42)\n","\n","print(\"Number of training samples:\",len(d_train))\n","print(\"Number of validation samples:\",len(d_valid))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Number of training samples: 229340\n","Number of validation samples: 2317\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IPWeo_x06oqK","colab_type":"code","outputId":"7c695e8b-9a79-41de-b7e0-1639691ddd95","executionInfo":{"status":"ok","timestamp":1556857628420,"user_tz":420,"elapsed":506,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["def numerized2text(numerized):\n","    \"\"\" Converts an integer sequence in the vocabulary into a string corresponding to the title.\n","    \n","        Arguments:\n","            numerized: List[int]  -- The list of vocabulary indices corresponding to the string\n","        Returns:\n","            title: str -- The string corresponding to the numerized input, without padding.\n","    \"\"\"\n","    #####\n","    # BEGIN YOUR CODE HERE \n","    # Recover each word from the vocabulary in the list of indices in numerized, using the vocabulary variable\n","    # Hint: Use the string.join() function to reconstruct a single string\n","    #####\n","    \n","    words = [vocabulary[int(num)] for num in numerized]\n","    converted_string = ' '.join(words)\n","    \n","    #####\n","    # END YOUR CODE HERE\n","    #####\n","    \n","    return converted_string\n","\n","entry = d_train[1001]\n","print(\"Reversing the numerized: \"+numerized2text(entry['numerized']))\n","print(\"From the `title` entry: \"+ entry['Joke'])"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Reversing the numerized: an old man shuffled slowly into an ice cream parlor. he ordered a banana split. the waitress asked , crushed nuts ? no , he said. arthritis . PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n","From the `title` entry: An old man shuffled slowly into an ice cream parlor. He ordered a banana split. The waitress asked, Crushed nuts? No, he said. Arthritis.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1HQ6wmgN645s","colab_type":"code","colab":{}},"source":["def build_batch(dataset, batch_size):\n","    \"\"\" Builds a batch of source and target elements from the dataset.\n","    \n","        Arguments:\n","            dataset: List[db_element] -- A list of dataset elements\n","            batch_size: int -- The size of the batch that should be created\n","        Returns:\n","            batch_input: List[List[int]] -- List of source sequences\n","            batch_target: List[List[int]] -- List of target sequences\n","            batch_target_mask: List[List[int]] -- List of target batch masks\n","    \"\"\"\n","    \n","    #####\n","    # BEGIN YOUR CODE HERE \n","    #####\n","    \n","    \n","    # We get a list of indices we will choose from the dataset.\n","    # The randint function uses a uniform distribution, giving equal probably to any entry\n","    # for each batch\n","    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n","    \n","    # Recover what the entries for the batch are\n","    batch = [dataset[i] for i in indices]\n","    \n","    # Get the raw numerized for this input, each element of the dataset has a 'numerized' key\n","    batch_numerized = np.asarray([db_element[\"numerized\"] for db_element in batch])\n","\n","    # Create an array of start_index that will be concatenated at position 1 for the input.\n","    # Should be of shape (batch_size, 1)\n","    start_tokens = np.zeros((batch_size, 1))\n","\n","    # Concatenate the start_tokens with the rest of the input\n","    # The np.concatenate function should be useful\n","    # The output should now be [batch_size, sequence_length+1]\n","    batch_input = np.concatenate((start_tokens, batch_numerized), axis=1)\n","\n","    # Remove the last word from each element in the batch\n","    # To restore the [batch_size, sequence_length] size\n","    batch_input = batch_input[:, :-1]\n","    \n","    # The target should be the un-shifted numerized input\n","    batch_target = batch_numerized\n","\n","    # The target-mask is a 0 or 1 filter to note which tokens are\n","    # padding or not, to give the loss, so the model doesn't get rewarded for\n","    # predicting PAD tokens.\n","    batch_target_mask = np.array([a['mask'] for a in batch])\n","    \n","    #####\n","    # END YOUR CODE HERE \n","    #####\n","        \n","    return batch_input, batch_target, batch_target_mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6uh3DK687AzR","colab_type":"code","colab":{}},"source":["# Using a basic RNN/LSTM for Language modeling\n","class LanguageModel():\n","    def __init__(self, input_length, vocab_size, rnn_size, learning_rate=1e-4):\n","        \n","        # Create the placeholders for the inputs:\n","        # All three placeholders should be of size [None, input_length]\n","        # Where None represents a variable batch_size, and input_length is the\n","        # maximal length of a sequence of words, after being padded.\n","        self.input_num = tf.placeholder(tf.int32, shape=[None, input_length])\n","        \n","        # my code\n","        self.targets = tf.placeholder(tf.int32, shape=[None, input_length])\n","        self.targets_mask = tf.placeholder(tf.bool, shape=[None, input_length])\n","\n","        # Create an embedding variable of shape [vocab_size, rnn_size]\n","        # That will map each word in our vocab into a vector of rnn_size size.\n","        embedding = tf.Variable(tf.random_uniform([vocab_size, rnn_size], -1.0, 1.0))\n","        # Use the tensorflow embedding_lookup function\n","        # To embed the input_num, using the embedding variable we've created\n","        input_emb = tf.nn.embedding_lookup(embedding, self.input_num)\n","\n","        # Create a an RNN or LSTM cell of rnn_size size.\n","        # Look into the tf.nn.rnn_cell documentation\n","        # You can optionally use Tensorflow Add-ons such as the MultiRNNCell, or the DropoutWrapper\n","        lm_cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n","        \n","        # add another cell for MultiRNNCell\n","        # hidden_units = [rnn_size] * 10 \n","        # hidden_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n) for n in hidden_units]\n","        # hidden_cells.append(lm_cell)\n","        # stacked_rnn_cell = MultiRNNCell(hidden_cells)\n","        \n","        # Use the dynamic_rnn function of Tensorflow to run the embedded inputs\n","        # using the lm_cell you've created, and obtain the outputs of the RNN cell.\n","        # You have created a cell, which represents a single block (column) of the RNN.\n","        # dynamic_rnn will \"copy\" the cell for each element in your sequence, runs the input you provide through the cell,\n","        # and returns the outputs and the states of the cell.\n","        outputs, states = tf.nn.dynamic_rnn(lm_cell, input_emb, dtype=tf.float32)\n","\n","        # Use a dense layer to project the outputs of the RNN cell into the size of the\n","        # vocabulary (vocab_size).\n","        # output_logits should be of shape [None,input_length,vocab_size]\n","        # You can look at the tf.layers.dense function\n","        self.output_logits = tf.layers.dense(inputs=outputs, units=vocab_size)\n","\n","        # Setup the loss: using the sparse_softmax_cross_entropy.\n","        # The logits are the output_logits we've computed.\n","        # The targets are the gold labels we are trying to match\n","        # Don't forget to use the targets_mask we have, so your loss is not off,\n","        # And your model doesn't get rewarded for predicting PAD tokens\n","        # You might have to cast the masks into float32. Look at the tf.cast function.\n","        weights = tf.cast(self.targets_mask, tf.float32)\n","        self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.targets,logits=self.output_logits, weights=weights)\n","\n","        # Setup an optimizer (SGD, RMSProp, Adam), you can find a list under tf.train.*\n","        # And provide it with a start learning rate.\n","\n","        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam')     \n","\n","        # We create a train_op that requires the optimizer we've created to minimize the\n","        # loss we've defined.\n","        # look for the optimizer.minimize function, define what should be miniminzed.\n","        # You can provide it with the provide an optional global_step parameter as well that keeps of how many\n","        # Optimizations steps have been run.\n","        \n","        self.global_step = tf.train.get_or_create_global_step()\n","        self.train_op = optimizer.minimize(self.loss, global_step=self.global_step)\n","        self.saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_opYPIx7DV4","colab_type":"code","outputId":"a5ca8f72-69f7-4e78-e87e-85cb3188c894","executionInfo":{"status":"ok","timestamp":1556857638187,"user_tz":420,"elapsed":1732,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":349}},"source":["# We can create our model,\n","# with parameters of our choosing.\n","\n","tf.reset_default_graph() # This is so that when you debug, you reset the graph each time you run this, in essence, cleaning the board\n","model = LanguageModel(input_length=input_length, vocab_size=vocab_size, rnn_size=256*2, learning_rate=1e-4)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From <ipython-input-31-d6d80a32e672>:24: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-31-d6d80a32e672>:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From <ipython-input-31-d6d80a32e672>:43: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c5xwj3_v7IhF","colab_type":"code","outputId":"588ba52c-0f6d-4cb9-bce0-739cbcf812bf","colab":{"base_uri":"https://localhost:8080/","height":18772},"executionInfo":{"status":"ok","timestamp":1556868189410,"user_tz":420,"elapsed":9045397,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}}},"source":["# DO NOT RUN THIS BLOCK IF YOU DON\"T WANT TO TRAIN THE NETWORK\n","\n","# Skeleton code\n","# You have to write your own training process to obtain a\n","# Good performing model on the validation set, and save it.\n","\n","experiment = root_folder+\"models/final_language_model\"\n","\n","with tf.Session() as sess:\n","    # Here is how you initialize weights of the model according to their\n","    # Initialization parameters.\n","    sess.run(tf.global_variables_initializer())\n","    \n","    # Here is how you restore the weights previously saved\n","    model.saver.restore(sess, experiment)\n","    \n","    epoch = 3\n","    batch_size = 64\n","    num_iter = epoch * len(d_train) // batch_size\n","    print(\"Total number of iterations is: \" + str(num_iter))\n","    \n","    eval_input, eval_target, eval_target_mask = build_batch(d_valid, 50)\n","    feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n","    eval_loss = sess.run(model.loss, feed_dict=feed)\n","    print(\"Evaluation set loss: \", eval_loss)\n","        \n","    for i in range(num_iter):\n","        # Here is how you obtain a batch:\n","        batch_input, batch_target, batch_target_mask = build_batch(d_train, batch_size)\n","        # Map the values to each tensor in a `feed_dict`\n","        feed = {model.input_num: batch_input, model.targets: batch_target, model.targets_mask: batch_target_mask}\n","\n","        # Obtain a single value of the loss for that batch.\n","        # !IMPORTANT! Don't forget to include the train_op to when using a batch from the training dataset\n","        # (d_train)\n","        # !MORE IMPORTANT! Don't use the train_op if you evaluate the loss on the validation set,\n","        # Otherwise, your network will overfit on your validation dataset.\n","\n","        step, train_loss, _ = sess.run([model.global_step, model.loss, model.train_op], feed_dict=feed)\n","\n","        if i % 20 == 0:\n","          print(\"step: \" + str(i))\n","        if i % 100 == 0:\n","            print(\"step: \" + str(i))\n","            print(\"train_loss: \" + str(train_loss))\n","            eval_input, eval_target, eval_target_mask = build_batch(d_valid, 50)\n","            feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n","            eval_loss_steps = sess.run(model.loss, feed_dict=feed)\n","            # if (eval_loss_steps < eval_loss):\n","            #  print(\"eval_loss decreases!\")\n","            eval_loss = eval_loss_steps\n","            print(\"Evaluation set loss: \", eval_loss)\n","            print(\"saving model weights ....\")\n","            model.saver.save(sess, experiment)\n","            print(\"saving model weights completed ....\")\n","            # else:\n","            #  print(\"eval_loss didn't decrease.\")\n","            #  print(\"half learning rate, make another model, reset to previous checkpoint\")\n","            #  # learning_rate /= 2\n","            #  # model = LanguageModel(input_length=input_length, vocab_size=vocab_size, rnn_size=256*4, learning_rate=learning_rate)\n","            #  model.saver.restore(sess, experiment)\n","    \n","    # Here is how you save the model weights\n","    model.saver.save(sess, experiment)\n","    \n","    # Here is how you restore the weights previously saved\n","    model.saver.restore(sess, experiment)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Project-Lion/models/final_language_model\n","Total number of iterations is: 10750\n","Evaluation set loss:  5.1346316\n","step: 0\n","step: 0\n","train_loss: 4.844065\n","Evaluation set loss:  4.7709723\n","saving model weights ....\n","saving model weights completed ....\n","step: 20\n","step: 40\n","step: 60\n","step: 80\n","step: 100\n","step: 100\n","train_loss: 4.7974916\n","Evaluation set loss:  4.790634\n","saving model weights ....\n","saving model weights completed ....\n","step: 120\n","step: 140\n","step: 160\n","step: 180\n","step: 200\n","step: 200\n","train_loss: 4.541914\n","Evaluation set loss:  5.088226\n","saving model weights ....\n","saving model weights completed ....\n","step: 220\n","step: 240\n","step: 260\n","step: 280\n","step: 300\n","step: 300\n","train_loss: 4.7916493\n","Evaluation set loss:  5.2291493\n","saving model weights ....\n","saving model weights completed ....\n","step: 320\n","step: 340\n","step: 360\n","step: 380\n","step: 400\n","step: 400\n","train_loss: 4.50705\n","Evaluation set loss:  4.878726\n","saving model weights ....\n","saving model weights completed ....\n","step: 420\n","step: 440\n","step: 460\n","step: 480\n","step: 500\n","step: 500\n","train_loss: 5.0137753\n","Evaluation set loss:  4.7348475\n","saving model weights ....\n","saving model weights completed ....\n","step: 520\n","step: 540\n","step: 560\n","step: 580\n","step: 600\n","step: 600\n","train_loss: 4.730289\n","Evaluation set loss:  4.9500837\n","saving model weights ....\n","saving model weights completed ....\n","step: 620\n","step: 640\n","step: 660\n","step: 680\n","step: 700\n","step: 700\n","train_loss: 4.408152\n","Evaluation set loss:  4.996792\n","saving model weights ....\n","saving model weights completed ....\n","step: 720\n","step: 740\n","step: 760\n","step: 780\n","step: 800\n","step: 800\n","train_loss: 4.878\n","Evaluation set loss:  5.0480247\n","saving model weights ....\n","saving model weights completed ....\n","step: 820\n","step: 840\n","step: 860\n","step: 880\n","step: 900\n","step: 900\n","train_loss: 4.544025\n","Evaluation set loss:  4.7439103\n","saving model weights ....\n","saving model weights completed ....\n","step: 920\n","step: 940\n","step: 960\n","step: 980\n","step: 1000\n","step: 1000\n","train_loss: 4.668212\n","Evaluation set loss:  4.785508\n","saving model weights ....\n","saving model weights completed ....\n","step: 1020\n","step: 1040\n","step: 1060\n","step: 1080\n","step: 1100\n","step: 1100\n","train_loss: 4.7571635\n","Evaluation set loss:  4.6793613\n","saving model weights ....\n","saving model weights completed ....\n","step: 1120\n","step: 1140\n","step: 1160\n","step: 1180\n","step: 1200\n","step: 1200\n","train_loss: 4.6480103\n","Evaluation set loss:  4.6762476\n","saving model weights ....\n","saving model weights completed ....\n","step: 1220\n","step: 1240\n","step: 1260\n","step: 1280\n","step: 1300\n","step: 1300\n","train_loss: 4.8377495\n","Evaluation set loss:  5.048223\n","saving model weights ....\n","saving model weights completed ....\n","step: 1320\n","step: 1340\n","step: 1360\n","step: 1380\n","step: 1400\n","step: 1400\n","train_loss: 4.9840255\n","Evaluation set loss:  5.1262403\n","saving model weights ....\n","saving model weights completed ....\n","step: 1420\n","step: 1440\n","step: 1460\n","step: 1480\n","step: 1500\n","step: 1500\n","train_loss: 4.591939\n","Evaluation set loss:  4.9160542\n","saving model weights ....\n","saving model weights completed ....\n","step: 1520\n","step: 1540\n","step: 1560\n","step: 1580\n","step: 1600\n","step: 1600\n","train_loss: 4.668232\n","Evaluation set loss:  5.014281\n","saving model weights ....\n","saving model weights completed ....\n","step: 1620\n","step: 1640\n","step: 1660\n","step: 1680\n","step: 1700\n","step: 1700\n","train_loss: 5.021374\n","Evaluation set loss:  4.732249\n","saving model weights ....\n","saving model weights completed ....\n","step: 1720\n","step: 1740\n","step: 1760\n","step: 1780\n","step: 1800\n","step: 1800\n","train_loss: 4.4792175\n","Evaluation set loss:  4.668671\n","saving model weights ....\n","saving model weights completed ....\n","step: 1820\n","step: 1840\n","step: 1860\n","step: 1880\n","step: 1900\n","step: 1900\n","train_loss: 4.881903\n","Evaluation set loss:  4.7404823\n","saving model weights ....\n","saving model weights completed ....\n","step: 1920\n","step: 1940\n","step: 1960\n","step: 1980\n","step: 2000\n","step: 2000\n","train_loss: 4.745655\n","Evaluation set loss:  5.0983644\n","saving model weights ....\n","saving model weights completed ....\n","step: 2020\n","step: 2040\n","step: 2060\n","step: 2080\n","step: 2100\n","step: 2100\n","train_loss: 4.998922\n","Evaluation set loss:  5.00161\n","saving model weights ....\n","saving model weights completed ....\n","step: 2120\n","step: 2140\n","step: 2160\n","step: 2180\n","step: 2200\n","step: 2200\n","train_loss: 4.8185067\n","Evaluation set loss:  5.0282006\n","saving model weights ....\n","saving model weights completed ....\n","step: 2220\n","step: 2240\n","step: 2260\n","step: 2280\n","step: 2300\n","step: 2300\n","train_loss: 4.7682056\n","Evaluation set loss:  4.9051557\n","saving model weights ....\n","saving model weights completed ....\n","step: 2320\n","step: 2340\n","step: 2360\n","step: 2380\n","step: 2400\n","step: 2400\n","train_loss: 4.7624683\n","Evaluation set loss:  4.782337\n","saving model weights ....\n","saving model weights completed ....\n","step: 2420\n","step: 2440\n","step: 2460\n","step: 2480\n","step: 2500\n","step: 2500\n","train_loss: 4.420316\n","Evaluation set loss:  4.8999796\n","saving model weights ....\n","saving model weights completed ....\n","step: 2520\n","step: 2540\n","step: 2560\n","step: 2580\n","step: 2600\n","step: 2600\n","train_loss: 4.682314\n","Evaluation set loss:  4.7746053\n","saving model weights ....\n","saving model weights completed ....\n","step: 2620\n","step: 2640\n","step: 2660\n","step: 2680\n","step: 2700\n","step: 2700\n","train_loss: 4.668403\n","Evaluation set loss:  4.8866644\n","saving model weights ....\n","saving model weights completed ....\n","step: 2720\n","step: 2740\n","step: 2760\n","step: 2780\n","step: 2800\n","step: 2800\n","train_loss: 4.8097353\n","Evaluation set loss:  4.5861955\n","saving model weights ....\n","saving model weights completed ....\n","step: 2820\n","step: 2840\n","step: 2860\n","step: 2880\n","step: 2900\n","step: 2900\n","train_loss: 4.659367\n","Evaluation set loss:  4.9160957\n","saving model weights ....\n","saving model weights completed ....\n","step: 2920\n","step: 2940\n","step: 2960\n","step: 2980\n","step: 3000\n","step: 3000\n","train_loss: 4.611441\n","Evaluation set loss:  4.8034925\n","saving model weights ....\n","saving model weights completed ....\n","step: 3020\n","step: 3040\n","step: 3060\n","step: 3080\n","step: 3100\n","step: 3100\n","train_loss: 4.6693125\n","Evaluation set loss:  5.0700426\n","saving model weights ....\n","saving model weights completed ....\n","step: 3120\n","step: 3140\n","step: 3160\n","step: 3180\n","step: 3200\n","step: 3200\n","train_loss: 4.583251\n","Evaluation set loss:  4.980787\n","saving model weights ....\n","saving model weights completed ....\n","step: 3220\n","step: 3240\n","step: 3260\n","step: 3280\n","step: 3300\n","step: 3300\n","train_loss: 4.8252206\n","Evaluation set loss:  5.108947\n","saving model weights ....\n","saving model weights completed ....\n","step: 3320\n","step: 3340\n","step: 3360\n","step: 3380\n","step: 3400\n","step: 3400\n","train_loss: 4.879678\n","Evaluation set loss:  4.94552\n","saving model weights ....\n","saving model weights completed ....\n","step: 3420\n","step: 3440\n","step: 3460\n","step: 3480\n","step: 3500\n","step: 3500\n","train_loss: 4.671498\n","Evaluation set loss:  4.7265716\n","saving model weights ....\n","saving model weights completed ....\n","step: 3520\n","step: 3540\n","step: 3560\n","step: 3580\n","step: 3600\n","step: 3600\n","train_loss: 4.3753815\n","Evaluation set loss:  5.0424185\n","saving model weights ....\n","saving model weights completed ....\n","step: 3620\n","step: 3640\n","step: 3660\n","step: 3680\n","step: 3700\n","step: 3700\n","train_loss: 4.5877886\n","Evaluation set loss:  4.8982925\n","saving model weights ....\n","saving model weights completed ....\n","step: 3720\n","step: 3740\n","step: 3760\n","step: 3780\n","step: 3800\n","step: 3800\n","train_loss: 4.552643\n","Evaluation set loss:  5.014938\n","saving model weights ....\n","saving model weights completed ....\n","step: 3820\n","step: 3840\n","step: 3860\n","step: 3880\n","step: 3900\n","step: 3900\n","train_loss: 4.636441\n","Evaluation set loss:  5.1423903\n","saving model weights ....\n","saving model weights completed ....\n","step: 3920\n","step: 3940\n","step: 3960\n","step: 3980\n","step: 4000\n","step: 4000\n","train_loss: 4.320278\n","Evaluation set loss:  5.2013764\n","saving model weights ....\n","saving model weights completed ....\n","step: 4020\n","step: 4040\n","step: 4060\n","step: 4080\n","step: 4100\n","step: 4100\n","train_loss: 4.779872\n","Evaluation set loss:  4.9071174\n","saving model weights ....\n","saving model weights completed ....\n","step: 4120\n","step: 4140\n","step: 4160\n","step: 4180\n","step: 4200\n","step: 4200\n","train_loss: 4.572158\n","Evaluation set loss:  4.8273506\n","saving model weights ....\n","saving model weights completed ....\n","step: 4220\n","step: 4240\n","step: 4260\n","step: 4280\n","step: 4300\n","step: 4300\n","train_loss: 4.564148\n","Evaluation set loss:  4.8383164\n","saving model weights ....\n","saving model weights completed ....\n","step: 4320\n","step: 4340\n","step: 4360\n","step: 4380\n","step: 4400\n","step: 4400\n","train_loss: 4.709177\n","Evaluation set loss:  4.835904\n","saving model weights ....\n","saving model weights completed ....\n","step: 4420\n","step: 4440\n","step: 4460\n","step: 4480\n","step: 4500\n","step: 4500\n","train_loss: 4.571221\n","Evaluation set loss:  4.9565153\n","saving model weights ....\n","saving model weights completed ....\n","step: 4520\n","step: 4540\n","step: 4560\n","step: 4580\n","step: 4600\n","step: 4600\n","train_loss: 4.397685\n","Evaluation set loss:  4.970511\n","saving model weights ....\n","saving model weights completed ....\n","step: 4620\n","step: 4640\n","step: 4660\n","step: 4680\n","step: 4700\n","step: 4700\n","train_loss: 4.5015736\n","Evaluation set loss:  4.717177\n","saving model weights ....\n","saving model weights completed ....\n","step: 4720\n","step: 4740\n","step: 4760\n","step: 4780\n","step: 4800\n","step: 4800\n","train_loss: 4.5682316\n","Evaluation set loss:  4.8688564\n","saving model weights ....\n","saving model weights completed ....\n","step: 4820\n","step: 4840\n","step: 4860\n","step: 4880\n","step: 4900\n","step: 4900\n","train_loss: 4.569766\n","Evaluation set loss:  5.091063\n","saving model weights ....\n","saving model weights completed ....\n","step: 4920\n","step: 4940\n","step: 4960\n","step: 4980\n","step: 5000\n","step: 5000\n","train_loss: 4.461823\n","Evaluation set loss:  4.8574076\n","saving model weights ....\n","saving model weights completed ....\n","step: 5020\n","step: 5040\n","step: 5060\n","step: 5080\n","step: 5100\n","step: 5100\n","train_loss: 4.5507584\n","Evaluation set loss:  4.608795\n","saving model weights ....\n","saving model weights completed ....\n","step: 5120\n","step: 5140\n","step: 5160\n","step: 5180\n","step: 5200\n","step: 5200\n","train_loss: 4.4036303\n","Evaluation set loss:  4.611188\n","saving model weights ....\n","saving model weights completed ....\n","step: 5220\n","step: 5240\n","step: 5260\n","step: 5280\n","step: 5300\n","step: 5300\n","train_loss: 4.4942183\n","Evaluation set loss:  4.8388147\n","saving model weights ....\n","saving model weights completed ....\n","step: 5320\n","step: 5340\n","step: 5360\n","step: 5380\n","step: 5400\n","step: 5400\n","train_loss: 4.1980367\n","Evaluation set loss:  4.712822\n","saving model weights ....\n","saving model weights completed ....\n","step: 5420\n","step: 5440\n","step: 5460\n","step: 5480\n","step: 5500\n","step: 5500\n","train_loss: 4.594052\n","Evaluation set loss:  4.667604\n","saving model weights ....\n","saving model weights completed ....\n","step: 5520\n","step: 5540\n","step: 5560\n","step: 5580\n","step: 5600\n","step: 5600\n","train_loss: 4.3083763\n","Evaluation set loss:  4.6321497\n","saving model weights ....\n","saving model weights completed ....\n","step: 5620\n","step: 5640\n","step: 5660\n","step: 5680\n","step: 5700\n","step: 5700\n","train_loss: 4.684062\n","Evaluation set loss:  4.5446587\n","saving model weights ....\n","saving model weights completed ....\n","step: 5720\n","step: 5740\n","step: 5760\n","step: 5780\n","step: 5800\n","step: 5800\n","train_loss: 4.6276207\n","Evaluation set loss:  4.9694448\n","saving model weights ....\n","saving model weights completed ....\n","step: 5820\n","step: 5840\n","step: 5860\n","step: 5880\n","step: 5900\n","step: 5900\n","train_loss: 4.508828\n","Evaluation set loss:  4.5790954\n","saving model weights ....\n","saving model weights completed ....\n","step: 5920\n","step: 5940\n","step: 5960\n","step: 5980\n","step: 6000\n","step: 6000\n","train_loss: 4.5870614\n","Evaluation set loss:  5.024369\n","saving model weights ....\n","saving model weights completed ....\n","step: 6020\n","step: 6040\n","step: 6060\n","step: 6080\n","step: 6100\n","step: 6100\n","train_loss: 4.8779993\n","Evaluation set loss:  4.808817\n","saving model weights ....\n","saving model weights completed ....\n","step: 6120\n","step: 6140\n","step: 6160\n","step: 6180\n","step: 6200\n","step: 6200\n","train_loss: 4.330972\n","Evaluation set loss:  5.116313\n","saving model weights ....\n","saving model weights completed ....\n","step: 6220\n","step: 6240\n","step: 6260\n","step: 6280\n","step: 6300\n","step: 6300\n","train_loss: 4.556608\n","Evaluation set loss:  4.7807074\n","saving model weights ....\n","saving model weights completed ....\n","step: 6320\n","step: 6340\n","step: 6360\n","step: 6380\n","step: 6400\n","step: 6400\n","train_loss: 4.4902434\n","Evaluation set loss:  4.5877533\n","saving model weights ....\n","saving model weights completed ....\n","step: 6420\n","step: 6440\n","step: 6460\n","step: 6480\n","step: 6500\n","step: 6500\n","train_loss: 4.456004\n","Evaluation set loss:  4.610993\n","saving model weights ....\n","saving model weights completed ....\n","step: 6520\n","step: 6540\n","step: 6560\n","step: 6580\n","step: 6600\n","step: 6600\n","train_loss: 4.5390677\n","Evaluation set loss:  4.8349376\n","saving model weights ....\n","saving model weights completed ....\n","step: 6620\n","step: 6640\n","step: 6660\n","step: 6680\n","step: 6700\n","step: 6700\n","train_loss: 4.512528\n","Evaluation set loss:  5.08617\n","saving model weights ....\n","saving model weights completed ....\n","step: 6720\n","step: 6740\n","step: 6760\n","step: 6780\n","step: 6800\n","step: 6800\n","train_loss: 4.342225\n","Evaluation set loss:  4.77539\n","saving model weights ....\n","saving model weights completed ....\n","step: 6820\n","step: 6840\n","step: 6860\n","step: 6880\n","step: 6900\n","step: 6900\n","train_loss: 4.6139627\n","Evaluation set loss:  4.9566407\n","saving model weights ....\n","saving model weights completed ....\n","step: 6920\n","step: 6940\n","step: 6960\n","step: 6980\n","step: 7000\n","step: 7000\n","train_loss: 4.457537\n","Evaluation set loss:  4.8473268\n","saving model weights ....\n","saving model weights completed ....\n","step: 7020\n","step: 7040\n","step: 7060\n","step: 7080\n","step: 7100\n","step: 7100\n","train_loss: 4.5055323\n","Evaluation set loss:  4.927066\n","saving model weights ....\n","saving model weights completed ....\n","step: 7120\n","step: 7140\n","step: 7160\n","step: 7180\n","step: 7200\n","step: 7200\n","train_loss: 4.5204134\n","Evaluation set loss:  4.7725596\n","saving model weights ....\n","saving model weights completed ....\n","step: 7220\n","step: 7240\n","step: 7260\n","step: 7280\n","step: 7300\n","step: 7300\n","train_loss: 4.235474\n","Evaluation set loss:  4.858845\n","saving model weights ....\n","saving model weights completed ....\n","step: 7320\n","step: 7340\n","step: 7360\n","step: 7380\n","step: 7400\n","step: 7400\n","train_loss: 4.758135\n","Evaluation set loss:  4.508388\n","saving model weights ....\n","saving model weights completed ....\n","step: 7420\n","step: 7440\n","step: 7460\n","step: 7480\n","step: 7500\n","step: 7500\n","train_loss: 4.408188\n","Evaluation set loss:  4.748838\n","saving model weights ....\n","saving model weights completed ....\n","step: 7520\n","step: 7540\n","step: 7560\n","step: 7580\n","step: 7600\n","step: 7600\n","train_loss: 4.5866947\n","Evaluation set loss:  4.6495247\n","saving model weights ....\n","saving model weights completed ....\n","step: 7620\n","step: 7640\n","step: 7660\n","step: 7680\n","step: 7700\n","step: 7700\n","train_loss: 4.3022313\n","Evaluation set loss:  4.7043457\n","saving model weights ....\n","saving model weights completed ....\n","step: 7720\n","step: 7740\n","step: 7760\n","step: 7780\n","step: 7800\n","step: 7800\n","train_loss: 4.3806257\n","Evaluation set loss:  4.787192\n","saving model weights ....\n","saving model weights completed ....\n","step: 7820\n","step: 7840\n","step: 7860\n","step: 7880\n","step: 7900\n","step: 7900\n","train_loss: 4.491149\n","Evaluation set loss:  4.6275287\n","saving model weights ....\n","saving model weights completed ....\n","step: 7920\n","step: 7940\n","step: 7960\n","step: 7980\n","step: 8000\n","step: 8000\n","train_loss: 4.5923324\n","Evaluation set loss:  4.5000076\n","saving model weights ....\n","saving model weights completed ....\n","step: 8020\n","step: 8040\n","step: 8060\n","step: 8080\n","step: 8100\n","step: 8100\n","train_loss: 4.5016055\n","Evaluation set loss:  5.0245614\n","saving model weights ....\n","saving model weights completed ....\n","step: 8120\n","step: 8140\n","step: 8160\n","step: 8180\n","step: 8200\n","step: 8200\n","train_loss: 4.580709\n","Evaluation set loss:  4.7792363\n","saving model weights ....\n","saving model weights completed ....\n","step: 8220\n","step: 8240\n","step: 8260\n","step: 8280\n","step: 8300\n","step: 8300\n","train_loss: 4.4806347\n","Evaluation set loss:  4.6265464\n","saving model weights ....\n","saving model weights completed ....\n","step: 8320\n","step: 8340\n","step: 8360\n","step: 8380\n","step: 8400\n","step: 8400\n","train_loss: 4.483171\n","Evaluation set loss:  4.600821\n","saving model weights ....\n","saving model weights completed ....\n","step: 8420\n","step: 8440\n","step: 8460\n","step: 8480\n","step: 8500\n","step: 8500\n","train_loss: 4.5100956\n","Evaluation set loss:  4.698136\n","saving model weights ....\n","saving model weights completed ....\n","step: 8520\n","step: 8540\n","step: 8560\n","step: 8580\n","step: 8600\n","step: 8600\n","train_loss: 4.5840874\n","Evaluation set loss:  4.9904284\n","saving model weights ....\n","saving model weights completed ....\n","step: 8620\n","step: 8640\n","step: 8660\n","step: 8680\n","step: 8700\n","step: 8700\n","train_loss: 4.3375335\n","Evaluation set loss:  4.2760925\n","saving model weights ....\n","saving model weights completed ....\n","step: 8720\n","step: 8740\n","step: 8760\n","step: 8780\n","step: 8800\n","step: 8800\n","train_loss: 4.5538926\n","Evaluation set loss:  5.0832267\n","saving model weights ....\n","saving model weights completed ....\n","step: 8820\n","step: 8840\n","step: 8860\n","step: 8880\n","step: 8900\n","step: 8900\n","train_loss: 4.225877\n","Evaluation set loss:  5.15722\n","saving model weights ....\n","saving model weights completed ....\n","step: 8920\n","step: 8940\n","step: 8960\n","step: 8980\n","step: 9000\n","step: 9000\n","train_loss: 4.2408876\n","Evaluation set loss:  4.738759\n","saving model weights ....\n","saving model weights completed ....\n","step: 9020\n","step: 9040\n","step: 9060\n","step: 9080\n","step: 9100\n","step: 9100\n","train_loss: 4.602129\n","Evaluation set loss:  4.57628\n","saving model weights ....\n","saving model weights completed ....\n","step: 9120\n","step: 9140\n","step: 9160\n","step: 9180\n","step: 9200\n","step: 9200\n","train_loss: 4.407934\n","Evaluation set loss:  4.747433\n","saving model weights ....\n","saving model weights completed ....\n","step: 9220\n","step: 9240\n","step: 9260\n","step: 9280\n","step: 9300\n","step: 9300\n","train_loss: 4.3414035\n","Evaluation set loss:  4.785869\n","saving model weights ....\n","saving model weights completed ....\n","step: 9320\n","step: 9340\n","step: 9360\n","step: 9380\n","step: 9400\n","step: 9400\n","train_loss: 4.4306726\n","Evaluation set loss:  4.8073816\n","saving model weights ....\n","saving model weights completed ....\n","step: 9420\n","step: 9440\n","step: 9460\n","step: 9480\n","step: 9500\n","step: 9500\n","train_loss: 4.1995254\n","Evaluation set loss:  4.6544704\n","saving model weights ....\n","saving model weights completed ....\n","step: 9520\n","step: 9540\n","step: 9560\n","step: 9580\n","step: 9600\n","step: 9600\n","train_loss: 4.7003417\n","Evaluation set loss:  4.561746\n","saving model weights ....\n","saving model weights completed ....\n","step: 9620\n","step: 9640\n","step: 9660\n","step: 9680\n","step: 9700\n","step: 9700\n","train_loss: 4.3484464\n","Evaluation set loss:  4.4921675\n","saving model weights ....\n","saving model weights completed ....\n","step: 9720\n","step: 9740\n","step: 9760\n","step: 9780\n","step: 9800\n","step: 9800\n","train_loss: 4.196483\n","Evaluation set loss:  4.898223\n","saving model weights ....\n","saving model weights completed ....\n","step: 9820\n","step: 9840\n","step: 9860\n","step: 9880\n","step: 9900\n","step: 9900\n","train_loss: 4.1508126\n","Evaluation set loss:  5.1603036\n","saving model weights ....\n","saving model weights completed ....\n","step: 9920\n","step: 9940\n","step: 9960\n","step: 9980\n","step: 10000\n","step: 10000\n","train_loss: 4.384887\n","Evaluation set loss:  4.848559\n","saving model weights ....\n","saving model weights completed ....\n","step: 10020\n","step: 10040\n","step: 10060\n","step: 10080\n","step: 10100\n","step: 10100\n","train_loss: 4.3292065\n","Evaluation set loss:  5.0523314\n","saving model weights ....\n","saving model weights completed ....\n","step: 10120\n","step: 10140\n","step: 10160\n","step: 10180\n","step: 10200\n","step: 10200\n","train_loss: 4.2608476\n","Evaluation set loss:  4.9636497\n","saving model weights ....\n","saving model weights completed ....\n","step: 10220\n","step: 10240\n","step: 10260\n","step: 10280\n","step: 10300\n","step: 10300\n","train_loss: 4.3212237\n","Evaluation set loss:  4.965724\n","saving model weights ....\n","saving model weights completed ....\n","step: 10320\n","step: 10340\n","step: 10360\n","step: 10380\n","step: 10400\n","step: 10400\n","train_loss: 4.287345\n","Evaluation set loss:  4.565524\n","saving model weights ....\n","saving model weights completed ....\n","step: 10420\n","step: 10440\n","step: 10460\n","step: 10480\n","step: 10500\n","step: 10500\n","train_loss: 4.556901\n","Evaluation set loss:  4.679854\n","saving model weights ....\n","saving model weights completed ....\n","step: 10520\n","step: 10540\n","step: 10560\n","step: 10580\n","step: 10600\n","step: 10600\n","train_loss: 4.296097\n","Evaluation set loss:  4.574335\n","saving model weights ....\n","saving model weights completed ....\n","step: 10620\n","step: 10640\n","step: 10660\n","step: 10680\n","step: 10700\n","step: 10700\n","train_loss: 4.207456\n","Evaluation set loss:  5.190036\n","saving model weights ....\n","saving model weights completed ....\n","step: 10720\n","step: 10740\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Project-Lion/models/final_language_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G26slr6H_fy3","colab_type":"code","colab":{}},"source":["df = pd.read_csv(root_folder + 'data/kaggle/shortjokes.csv')\n","df = df.loc[df[\"Joke\"] != \"\"]\n","data = clean_data(df[\"Joke\"].tolist(), keep_punc=\"?\")\n","qa_all = [[s.strip() for s in list(filter(None, joke.split(\"?\")))] for joke in data if len(list(filter(None, joke.split(\"?\")))) == 2]\n","min_qlen, max_qlen = 3, 15\n","min_alen, max_alen = 3, 15\n","qa = [(q, a) for q, a in qa_all if len(q.split()) >= min_qlen and len(q.split()) <= max_qlen and len(a.split()) >= min_alen and len(a.split()) <= max_alen]\n","questions = [q for q, a in qa]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkEaXIOrAOMW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"fce667a4-a4bf-44e0-a3b5-baab75096dfb","executionInfo":{"status":"ok","timestamp":1556870775889,"user_tz":420,"elapsed":877,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}}},"source":["questions_500 = questions[:500]\n","questions_500[:5]"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['why cant barbie get pregnant',\n"," 'why was the musician arrested',\n"," 'did you hear about the guy who blew his entire lottery winnings on a limousine',\n"," 'what do you do if a bird shits on your car',\n"," 'what should you do before criticizing pacman']"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"A1ipeOyW7Ml2","colab_type":"code","outputId":"3cc9d124-3bae-4e27-f9b9-998ddab4a0a2","executionInfo":{"status":"ok","timestamp":1556872436361,"user_tz":420,"elapsed":1647222,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["model_file = root_folder+\"models/final_language_model\"\n","\n","# Saving generated text to a file\n","with tf.Session() as sess:\n","    model.saver.restore(sess, model_file)\n","\n","    # Here are some headline starters.\n","    # They're all about tech companies, because\n","    # That is what is in our dataset\n","    headline_starters = [\"do you know\", \"how\", \"knock knock\", \"why did the chicken\", \"universe\", \"college\"]\n","    \n","    with open(root_folder + \"LM_output_jokes.txt\", \"w\") as text_file:\n","      for headline_starter in questions_500:\n","          #print(\"===================\")\n","          #print(\"Generating Jokes starting with: \"+headline_starter)\n","          print(headline_starter, file = text_file)\n","\n","          # Tokenize and numerize the headline. Put the numerized headline\n","          # beginning in `current_build`\n","          tokenized = tokenizer.word_tokenizer(headline_starter)\n","          current_build = [startI] + numerize_sequence(tokenized)\n","          \n","          i = 0\n","          while len(current_build) < input_length:\n","              # Pad the current_build into a input_length vector.\n","              # We do this so that it can be processed by our LanguageModel class\n","              current_padded = current_build[:input_length] + [padI] * (input_length - len(current_build))\n","              current_padded = np.array([current_padded])\n","\n","              # Obtain the logits for the current padded sequence\n","              # This involves obtaining the output_logits from our model,\n","              # and not the loss like we have done so far\n","              feed = {model.input_num: current_padded}\n","              logits = sess.run(model.output_logits, feed_dict=feed)\n","\n","              # Obtain the row of logits that interest us, the logits for the last non-pad\n","              # inputs\n","              last_index = len(current_build) - 1\n","              last_logits = logits[0][last_index]\n","\n","              # Find the highest scoring word in the last_logits\n","              # array. The np.argmax function should be useful.\n","              # Append this word to our current build\n","              current_build.append(np.argmax(last_logits))\n","              if vocabulary[int(np.argmax(last_logits))] != '.':\n","                i += 1\n","\n","          # Go from the current_build of word_indices\n","          # To the headline (string) produced. This should involve\n","          # the vocabulary, and a string merger.\n","          produced_sentence = numerized2text(current_build)\n","          \n","          question = headline_starter.split(\" \")\n","          produced_sentence = produced_sentence.split(\" \")\n","          text_generated = produced_sentence[(len(question) + 2):len(question) + 1 + i]\n","          sentence = ' '.join(text_generated)\n","          print(sentence, file = text_file)\n","          print(\"\", file = text_file)"],"execution_count":72,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Project-Lion/models/final_language_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uey_7XKbqu_-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"14aI8LvC7dLT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":921},"outputId":"fcfe4e10-3100-4af2-ce4c-85836bfbf9c7","executionInfo":{"status":"ok","timestamp":1556868544211,"user_tz":420,"elapsed":35332,"user":{"displayName":"ALLEN CHEN","photoUrl":"","userId":"13117472557552131577"}}},"source":["model_file = root_folder+\"models/final_language_model\"\n","\n","with tf.Session() as sess:\n","    model.saver.restore(sess, model_file)\n","\n","    # Here are some headline starters.\n","    # They're all about tech companies, because\n","    # That is what is in our dataset\n","    headline_starters = [\"do you know\", \"how\", \"knock knock\", \"why did the chicken\", \"universe\", \"college\"]\n","    \n","    for headline_starter in questions_500:\n","        print(\"===================\")\n","        print(\"Generating Jokes starting with: \"+headline_starter)\n","        print(headline_starter)\n","\n","        # Tokenize and numerize the headline. Put the numerized headline\n","        # beginning in `current_build`\n","        tokenized = tokenizer.word_tokenizer(headline_starter)\n","        current_build = [startI] + numerize_sequence(tokenized)\n","\n","        while len(current_build) < input_length:\n","            # Pad the current_build into a input_length vector.\n","            # We do this so that it can be processed by our LanguageModel class\n","            current_padded = current_build[:input_length] + [padI] * (input_length - len(current_build))\n","            current_padded = np.array([current_padded])\n","\n","            # Obtain the logits for the current padded sequence\n","            # This involves obtaining the output_logits from our model,\n","            # and not the loss like we have done so far\n","            feed = {model.input_num: current_padded}\n","            logits = sess.run(model.output_logits, feed_dict=feed)\n","\n","            # Obtain the row of logits that interest us, the logits for the last non-pad\n","            # inputs\n","            last_index = len(current_build) - 1\n","            last_logits = logits[0][last_index]\n","\n","            # Find the highest scoring word in the last_logits\n","            # array. The np.argmax function should be useful.\n","            # Append this word to our current build\n","            current_build.append(np.argmax(last_logits))\n","\n","        # Go from the current_build of word_indices\n","        # To the headline (string) produced. This should involve\n","        # the vocabulary, and a string merger.\n","        produced_sentence = numerized2text(current_build)\n","        #print(\"\\n\", file = text_file)\n","        \n","        print(produced_sentence)\n","        print(\"\")"],"execution_count":44,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Project-Lion/models/final_language_model\n","===================\n","Generating Jokes starting with: why cant barbie get pregnant\n","why cant barbie get pregnant\n","<START> why cant barbie get pregnant ? because they can't even . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: why was the musician arrested\n","why was the musician arrested\n","<START> why was the musician arrested ? he was a fungi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: did you hear about the guy who blew his entire lottery winnings on a limousine\n","did you hear about the guy who blew his entire lottery winnings on a limousine\n","<START> did you hear about the guy who blew his entire lottery winnings on a limousine ? he was a little hoarse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: what do you do if a bird shits on your car\n","what do you do if a bird shits on your car\n","<START> what do you do if a bird shits on your car ? you get it off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: what should you do before criticizing pacman\n","what should you do before criticizing pacman\n","<START> what should you do before criticizing pacman ? a : the p . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: whats the difference between an illegal mexican and an autonomous robot\n","whats the difference between an illegal mexican and an autonomous robot\n","<START> whats the difference between an illegal mexican and an autonomous robot ? one is a cunning array of stunts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: what did arnold schwarzenegger say at the abortion clinic\n","what did arnold schwarzenegger say at the abortion clinic\n","<START> what did arnold schwarzenegger say at the abortion clinic ? \" i don't know , i don't know , but i don't know , but i don't know , but i don't know , but i don't know , but i don't know , but i don't know , but i don't know , but i don't know , i don't know , but i don't know , but i don't know , i don't know , but i don't know . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: thanksgiving joke what does miley cyrus eat for thanksgiving\n","thanksgiving joke what does miley cyrus eat for thanksgiving\n","<START> thanksgiving joke what does miley cyrus eat for thanksgiving ? a buck . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: why do you never see elephants hiding in trees\n","why do you never see elephants hiding in trees\n","<START> why do you never see elephants hiding in trees ? because they are always in the middle of the closet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n","===================\n","Generating Jokes starting with: how did the blonde die raking leaves\n","how did the blonde die raking leaves\n","<START> how did the blonde die raking leaves ? she was in the kitchen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g97esSLLl-4B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}